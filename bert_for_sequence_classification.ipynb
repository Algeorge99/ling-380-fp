{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1f607e7f",
      "metadata": {
        "id": "1f607e7f"
      },
      "source": [
        "Model: BERT tokenizer + RNN (Linear, Tanh) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a03a719c",
      "metadata": {
        "id": "a03a719c"
      },
      "source": [
        "Incorporated some code from:https://www.kaggle.com/barelydedicated/yelp-review-predictions-using-huggingface-bert/notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa2c4dc8",
      "metadata": {
        "id": "aa2c4dc8"
      },
      "outputs": [],
      "source": [
        "# Set to your own working directory (don't need in colab)\n",
        "#%cd /Users/kelsey/yelp"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79467b6c",
      "metadata": {
        "id": "79467b6c"
      },
      "source": [
        "### Installs & Imports: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d536b36d",
      "metadata": {
        "id": "d536b36d"
      },
      "outputs": [],
      "source": [
        "# Installed the following, which I did not have \n",
        "\n",
        "!pip install transformers==3.5.1\n",
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece\n",
        "!pip install bert\n",
        "!pip install bert-tensorflow\n",
        "!pip install seaborn\n",
        "!pip install tensorflow\n",
        "!pip install tensorflow_hub\n",
        "# FOR COLAB ONLY\n",
        "!pip install torch==1.4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "657b4b49",
      "metadata": {
        "id": "657b4b49"
      },
      "outputs": [],
      "source": [
        "# Import all of the following: \n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime\n",
        "from transformers import *\n",
        "import torch\n",
        "from transformers.data.processors.utils import InputExample\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.notebook import tqdm, tnrange \n",
        "import time\n",
        "import bert\n",
        "from bert import optimization\n",
        "from bert import tokenization\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82531298",
      "metadata": {
        "id": "82531298"
      },
      "source": [
        "### The Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5a6f2018",
      "metadata": {
        "id": "5a6f2018"
      },
      "outputs": [],
      "source": [
        "# Upload .csv of dataset (cleaning is in another notebook)\n",
        "yelp = pd.read_csv(\"yelp_clean.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0bd6e525",
      "metadata": {
        "id": "0bd6e525"
      },
      "outputs": [],
      "source": [
        "# BERT tokenizer can't take any NaN's, so fill with string \"no text\"\n",
        "yelp = yelp.fillna(\"no text\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yelp.columns"
      ],
      "metadata": {
        "id": "lDJoFs9CqpI4"
      },
      "id": "lDJoFs9CqpI4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset of data for messing w parameters: Comment out when not using\n",
        "# This is 1% of the data, which is 1.5k rows \n",
        "#yelp = yelp.sample(frac = 0.4, replace = False, random_state = 42)"
      ],
      "metadata": {
        "id": "6JVZ_cG6SvkT"
      },
      "id": "6JVZ_cG6SvkT",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yelp.shape"
      ],
      "metadata": {
        "id": "16_Hc0mWiAeb"
      },
      "id": "16_Hc0mWiAeb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6763b555",
      "metadata": {
        "id": "6763b555"
      },
      "outputs": [],
      "source": [
        "# Train-test split\n",
        "x = yelp['text'].tolist()\n",
        "y = yelp['stars'].tolist()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, \n",
        "                                                    random_state=42)\n",
        "\n",
        "data_train = pd.DataFrame()\n",
        "data_train['text'] = x_train\n",
        "data_train['stars'] = y_train\n",
        "\n",
        "data_test = pd.DataFrame()\n",
        "data_test['text'] = x_test\n",
        "data_test['stars'] = y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56fcaa46",
      "metadata": {
        "id": "56fcaa46"
      },
      "source": [
        "### The Model "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3da5adcb",
      "metadata": {
        "id": "3da5adcb"
      },
      "source": [
        "Create the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76b9a911",
      "metadata": {
        "id": "76b9a911"
      },
      "outputs": [],
      "source": [
        "# Create tokenizer using pretrained BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Create the model \n",
        "hidden_size = 200\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-cased', \n",
        "                                                      num_labels=hidden_size)\n",
        "\n",
        "# Add Tanh Layer\n",
        "model.classifier.add_module('bert_activation', nn.Tanh())\n",
        "\n",
        "# Add Linear Layer \n",
        "model.classifier.add_module('prediction', nn.Linear(hidden_size, 5))\n",
        "\n",
        "# Print # of parameters\n",
        "FINE_TUNE = True\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'Total model trainable parameters {count_parameters(model)}')\n",
        "if FINE_TUNE:\n",
        "    for param in model.bert.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    for param in model.classifier.parameters():\n",
        "        param.requires_grad = True\n",
        "    print(f'Total head trainable parameters {count_parameters(model)}')\n",
        "model.cuda();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f7d3000",
      "metadata": {
        "id": "6f7d3000"
      },
      "outputs": [],
      "source": [
        "# Check to see what the model created\n",
        "model.classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "179fc7b4",
      "metadata": {
        "id": "179fc7b4"
      },
      "outputs": [],
      "source": [
        "# Check the tokenizer on a single sentence \n",
        "tokenized = tokenizer.tokenize('I have always loved the food here')\n",
        "print(tokenized)\n",
        "print(tokenizer.encode(tokenized, add_special_tokens=False))\n",
        "print(tokenizer.encode(tokenized, add_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "01332e15",
      "metadata": {
        "id": "01332e15"
      },
      "outputs": [],
      "source": [
        "# Create \"features\" function \n",
        "def get_features(df, text_col, label_col):\n",
        "    l = [InputExample(guid=idx, text_a=df.loc[idx, text_col], label=df.loc[idx, label_col]) for \n",
        "       idx, row in tqdm(df.iterrows(), total=df.shape[0])]\n",
        "    features = glue_convert_examples_to_features(examples=l, \n",
        "                                    tokenizer=tokenizer,\n",
        "                                    max_length=300,\n",
        "                                    label_list = df[label_col].values,\n",
        "                                    output_mode='regression')\n",
        "\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
        "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
        "    all_labels = torch.tensor([f.label-1 for f in features], dtype=torch.long)\n",
        "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_labels)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d6138fb",
      "metadata": {
        "id": "5d6138fb"
      },
      "outputs": [],
      "source": [
        "# Apply \"features\" function to training set and test set\n",
        "train_dataset = get_features(data_train, 'text', 'stars')\n",
        "test_dataset = get_features(data_test, 'text', 'stars')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "fnvAifos-HES",
      "metadata": {
        "id": "fnvAifos-HES"
      },
      "outputs": [],
      "source": [
        " # Train-test split again into train and eval\n",
        "val_idx, train_idx = train_test_split(np.arange(len(train_dataset)), \n",
        "                                      random_state=4, train_size=0.1)\n",
        "total_size = len(train_dataset)\n",
        "val_dataset = TensorDataset(*train_dataset[val_idx])\n",
        "train_dataset = TensorDataset(*train_dataset[train_idx])\n",
        "assert total_size == len(val_dataset) + len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "Euv7FFsc-uDm",
      "metadata": {
        "id": "Euv7FFsc-uDm"
      },
      "outputs": [],
      "source": [
        "# Create model + print\n",
        "model(input_ids=train_dataset[:2][0].cuda(), \n",
        "      attention_mask=train_dataset[:2][1].cuda(), \n",
        "      labels=train_dataset[:2][2].cuda());  \n",
        "#model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab406b23",
      "metadata": {
        "id": "ab406b23"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "baOa_jvc-v62",
      "metadata": {
        "id": "baOa_jvc-v62"
      },
      "outputs": [],
      "source": [
        "# Setup for training: select batch size, when to print gradient, num epochs, learning rate, optimizer\n",
        "\n",
        "batch_size = 16\n",
        "gradient_every = 32\n",
        "assert batch_size <= gradient_every and gradient_every % batch_size == 0\n",
        "\n",
        "accumulation_steps = gradient_every//batch_size\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size*2, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size*2, shuffle=False)\n",
        "\n",
        "epochs = 5\n",
        "lr = 0.002\n",
        "optimizer = AdamW(model.classifier.parameters(), lr=lr)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "_ydDhV4p-ymc",
      "metadata": {
        "id": "_ydDhV4p-ymc"
      },
      "outputs": [],
      "source": [
        "# Initialize losses \n",
        "tr_losses = []\n",
        "v_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df_lhTcT-02w",
      "metadata": {
        "id": "df_lhTcT-02w"
      },
      "outputs": [],
      "source": [
        "# TRAINING\n",
        "for epoch in tnrange(epochs, desc='epoch'):\n",
        "    \"\"\" Training stage \"\"\"\n",
        "    # initialize epoch loss\n",
        "    epoch_tr_losses = []\n",
        "    print(f'epoch {epoch+1}')\n",
        "    for k, (input_ids, attention_mask, labels) in enumerate(tqdm(train_dataloader, \n",
        "                                                                 total=len(train_dataloader), desc='batch')):\n",
        "        feed_dict = {'input_ids': input_ids.cuda(), \n",
        "                     'attention_mask': attention_mask.cuda(), \n",
        "                     'labels': labels.cuda()} \n",
        "        \n",
        "        loss, _ = model(**feed_dict)\n",
        "\n",
        "        # gradient accumulation\n",
        "        epoch_tr_losses.append(loss.item())\n",
        "        loss = loss/accumulation_steps\n",
        "        loss.backward()\n",
        "        if (k + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            model.zero_grad()\n",
        "\n",
        "    tr_losses.append(np.mean(epoch_tr_losses))\n",
        "    print(f'train NLL loss: {np.mean(epoch_tr_losses)}')\n",
        "  \n",
        "    \"\"\" Validation stage \"\"\"\n",
        "    epoch_v_losses = [] \n",
        "    with torch.no_grad():\n",
        "        for k, (input_ids, attention_mask, labels) in enumerate(tqdm(val_dataloader, \n",
        "                                                                     total=len(val_dataloader), desc='val batch')):\n",
        "            feed_dict = {'input_ids': input_ids.cuda(), \n",
        "                         'attention_mask': attention_mask.cuda(), \n",
        "                         'labels': labels.cuda()}  \n",
        "\n",
        "            loss, pred = model(**feed_dict)\n",
        "            epoch_v_losses.append(loss.item())\n",
        "        v_losses.append(np.mean(epoch_v_losses))\n",
        "    print(f'validation BCE loss: {np.mean(epoch_v_losses)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0db30e2",
      "metadata": {
        "id": "f0db30e2"
      },
      "source": [
        "### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cvVluruI_cG",
      "metadata": {
        "id": "2cvVluruI_cG"
      },
      "outputs": [],
      "source": [
        "# Initialize batch predictions + actual \n",
        "batch_predictions, batch_actual = [], []\n",
        "\n",
        "# Fill in batch predictions + actual \n",
        "with torch.no_grad():\n",
        "    for k, (input_ids, attention_mask, labels) in enumerate(\n",
        "        tqdm(test_dataloader, total=len(test_dataloader), desc='val batch')):\n",
        "        feed_dict = {'input_ids': input_ids.cuda(),\n",
        "                     'attention_mask': attention_mask.cuda()}\n",
        "        \n",
        "        pred = model(**feed_dict)[0].cpu()\n",
        "        batch_predictions.append(pred.numpy())\n",
        "        batch_actual.append(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "1H2CcP1nJIv_",
      "metadata": {
        "id": "1H2CcP1nJIv_"
      },
      "outputs": [],
      "source": [
        "# Total predictions + actual \n",
        "predictions = np.array([i for k in batch_predictions for i in k ])\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "actual = np.array([i for k in batch_actual for i in k ])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy on Test Data \n",
        "sum(predictions == actual)/len(actual)"
      ],
      "metadata": {
        "id": "2m0HebjuDvfO"
      },
      "id": "2m0HebjuDvfO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy on Test Data allowing +/- one star \n",
        "(sum(predictions == actual) + sum(predictions == (actual + 1)) + sum(\n",
        "    predictions == (actual - 1)))/len(actual)"
      ],
      "metadata": {
        "id": "VmjDXnm8TN7J"
      },
      "id": "VmjDXnm8TN7J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "hNb3oCsHJPUF",
      "metadata": {
        "id": "hNb3oCsHJPUF"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix function\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uyCCUCupJUS0",
      "metadata": {
        "id": "uyCCUCupJUS0"
      },
      "outputs": [],
      "source": [
        "# Compute \n",
        "confusion_mtx = confusion_matrix(actual, predictions) \n",
        "\n",
        "# Plot \n",
        "plot_confusion_matrix(confusion_mtx, classes = range(1,6))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Linguistic Examples"
      ],
      "metadata": {
        "id": "rBtZNyEE5PPA"
      },
      "id": "rBtZNyEE5PPA"
    },
    {
      "cell_type": "code",
      "source": [
        "examples = pd.read_csv(\"example_sentences - examples_combined.csv\")\n",
        "examples = examples.fillna(\"no text\")"
      ],
      "metadata": {
        "id": "qvz1PIHca98N"
      },
      "execution_count": 25,
      "outputs": [],
      "id": "qvz1PIHca98N"
    },
    {
      "cell_type": "code",
      "source": [
        "examples.columns"
      ],
      "metadata": {
        "id": "wIA9sYbdbOzh"
      },
      "execution_count": null,
      "outputs": [],
      "id": "wIA9sYbdbOzh"
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide into four types of examples\n",
        "examples_qualifier = examples[examples['qualifier'] == 1]\n",
        "examples_contradictory = examples[examples['contradictory'] == 1]\n",
        "examples_confusing = examples[examples['confusing'] == 1]\n",
        "examples_figurative = examples[examples['figurative'] == 1]"
      ],
      "metadata": {
        "id": "UtKAIXuvbHsO"
      },
      "execution_count": 27,
      "outputs": [],
      "id": "UtKAIXuvbHsO"
    },
    {
      "cell_type": "code",
      "source": [
        "# create features + dataloader just like for train & test data\n",
        "examples_qualifier_dataset = get_features(examples_qualifier, 'text', 'stars')\n",
        "examples_qualifier_dataloader = DataLoader(examples_qualifier_dataset)\n",
        "\n",
        "examples_contradictory_dataset = get_features(examples_contradictory, 'text', 'stars')\n",
        "examples_contradictory_dataloader = DataLoader(examples_contradictory_dataset)\n",
        "\n",
        "examples_confusing_dataset = get_features(examples_confusing, 'text', 'stars')\n",
        "examples_confusing_dataloader = DataLoader(examples_confusing_dataset)\n",
        "\n",
        "examples_figurative_dataset = get_features(examples_figurative, 'text', 'stars')\n",
        "examples_figurative_dataloader = DataLoader(examples_figurative_dataset)"
      ],
      "metadata": {
        "id": "H_u7Gx0MbhiX"
      },
      "execution_count": null,
      "outputs": [],
      "id": "H_u7Gx0MbhiX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "predictions_qualifier, pred_actual0 = [], []\n",
        "with torch.no_grad():\n",
        "    for k, (input_ids, attention_mask, labels) in enumerate(\n",
        "        tqdm(examples_qualifier_dataloader, total=len(examples_qualifier_dataloader), \n",
        "             desc = \"example\")): \n",
        "        feed_dict = {'input_ids': input_ids.cuda(),\n",
        "                     'attention_mask': attention_mask.cuda()}\n",
        "        \n",
        "        pred0 = model(**feed_dict)[0].cpu()\n",
        "        predictions_qualifier.append(pred0.numpy())"
      ],
      "metadata": {
        "id": "kdhM4vfLZPoX"
      },
      "id": "kdhM4vfLZPoX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_contradictory, pred_actual1 = [], []\n",
        "with torch.no_grad():\n",
        "    for k, (input_ids, attention_mask, labels) in enumerate(\n",
        "        tqdm(examples_contradictory_dataloader, total=len(examples_contradictory_dataloader), \n",
        "             desc = \"example\")): \n",
        "        feed_dict = {'input_ids': input_ids.cuda(),\n",
        "                     'attention_mask': attention_mask.cuda()}\n",
        "        \n",
        "        pred = model(**feed_dict)[0].cpu()\n",
        "        predictions_contradictory.append(pred.numpy())\n",
        "\n",
        "predictions_confusing, pred_actual2 = [], []\n",
        "with torch.no_grad():\n",
        "    for k, (input_ids, attention_mask, labels) in enumerate(\n",
        "        tqdm(examples_confusing_dataloader, total=len(examples_confusing_dataloader), \n",
        "             desc = \"example\")): \n",
        "        feed_dict = {'input_ids': input_ids.cuda(),\n",
        "                     'attention_mask': attention_mask.cuda()}\n",
        "        \n",
        "        pred = model(**feed_dict)[0].cpu()\n",
        "        predictions_confusing.append(pred.numpy())\n",
        "\n",
        "\n",
        "predictions_figurative, pred_actual3 = [], []\n",
        "with torch.no_grad():\n",
        "    for k, (input_ids, attention_mask, labels) in enumerate(\n",
        "        tqdm(examples_figurative_dataloader, total=len(examples_figurative_dataloader), \n",
        "             desc = \"example\")): \n",
        "        feed_dict = {'input_ids': input_ids.cuda(),\n",
        "                     'attention_mask': attention_mask.cuda()}\n",
        "        \n",
        "        pred = model(**feed_dict)[0].cpu()\n",
        "        predictions_figurative.append(pred.numpy())"
      ],
      "metadata": {
        "id": "dNFFtTEzcO_X"
      },
      "execution_count": null,
      "outputs": [],
      "id": "dNFFtTEzcO_X"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create predictions vectors & add to dataframes\n",
        "predictions_qualifier1  = np.array([i for k in predictions_qualifier for i in k])\n",
        "examples_qualifier['predictions'] = np.argmax(predictions_qualifier1, axis = 1) + 1\n",
        "\n",
        "predictions_contradictory1  = np.array([i for k in predictions_contradictory for i in k])\n",
        "examples_contradictory['predictions'] = np.argmax(predictions_contradictory1, axis = 1) + 1\n",
        "\n",
        "predictions_confusing1  = np.array([i for k in predictions_confusing for i in k])\n",
        "examples_confusing['predictions'] = np.argmax(predictions_confusing1, axis = 1) + 1\n",
        "\n",
        "predictions_figurative1  = np.array([i for k in predictions_figurative for i in k])\n",
        "examples_figurative['predictions'] = np.argmax(predictions_figurative1, axis = 1) + 1"
      ],
      "metadata": {
        "id": "ypHRcbPnck5P"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ypHRcbPnck5P"
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at matrices\n",
        "examples_qualifier[['text', 'stars', 'predictions']]"
      ],
      "metadata": {
        "id": "zCnNzuNgdp5F"
      },
      "execution_count": null,
      "outputs": [],
      "id": "zCnNzuNgdp5F"
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute \n",
        "confusion_matrix_qualifier = confusion_matrix(examples_qualifier[\"stars\"], \n",
        "                                              examples_qualifier[\"predictions\"])\n",
        "\n",
        "# Plot \n",
        "plot_confusion_matrix(confusion_matrix_qualifier, classes = range(1,6))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nNRJ8RQqeBFi"
      },
      "execution_count": null,
      "outputs": [],
      "id": "nNRJ8RQqeBFi"
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute \n",
        "confusion_matrix_contradictory = confusion_matrix(examples_contradictory[\"stars\"], \n",
        "                                              examples_contradictory[\"predictions\"])\n",
        "\n",
        "# Plot \n",
        "plot_confusion_matrix(confusion_matrix_contradictory, classes = [1, 2, 3, 4, 5])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qGiTXd3seB_v"
      },
      "execution_count": null,
      "outputs": [],
      "id": "qGiTXd3seB_v"
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute \n",
        "confusion_matrix_confusing = confusion_matrix(examples_confusing[\"stars\"], \n",
        "                                              examples_confusing[\"predictions\"])\n",
        "\n",
        "# Plot \n",
        "plot_confusion_matrix(confusion_matrix_confusing, classes = [1, 2, 3, 4, 5])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "---pkYIMeCgJ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "---pkYIMeCgJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute \n",
        "confusion_matrix_figurative = confusion_matrix(examples_figurative[\"stars\"], \n",
        "                                              examples_figurative[\"predictions\"])\n",
        "\n",
        "# Plot \n",
        "plot_confusion_matrix(confusion_matrix_figurative, classes = [1, 2, 3, 4, 5])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A9FI7BvDeem7"
      },
      "execution_count": null,
      "outputs": [],
      "id": "A9FI7BvDeem7"
    },
    {
      "cell_type": "code",
      "source": [
        "sum(examples_confusing[\"stars\"] == examples_confusing[\"predictions\"])/len(examples_confusing[\"stars\"])"
      ],
      "metadata": {
        "id": "PJ90B4Uofuj4"
      },
      "id": "PJ90B4Uofuj4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(examples_contradictory[\"stars\"] == examples_contradictory[\"predictions\"])/len(examples_contradictory[\"stars\"])"
      ],
      "metadata": {
        "id": "fvicAkddKtc8"
      },
      "id": "fvicAkddKtc8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(examples_qualifier[\"stars\"] == examples_qualifier[\"predictions\"])/len(examples_qualifier[\"stars\"])"
      ],
      "metadata": {
        "id": "mT7W52HKKuuH"
      },
      "id": "mT7W52HKKuuH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(examples_figurative[\"stars\"] == examples_figurative[\"predictions\"])/len(examples_figurative[\"stars\"])"
      ],
      "metadata": {
        "id": "8YeUge5tKvQP"
      },
      "id": "8YeUge5tKvQP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_results = pd.ExcelWriter('example_sentences.xls')\n",
        "\n",
        "# Write each dataframe to a different worksheet.\n",
        "examples_qualifier.to_excel(example_results, sheet_name='qualifiers')\n",
        "examples_confusing.to_excel(example_results, sheet_name='confusing')\n",
        "examples_contradictory.to_excel(example_results, sheet_name='contradictory')\n",
        "examples_figurative.to_excel(example_results, sheet_name = 'figurative')\n",
        "\n",
        "example_results.save()"
      ],
      "metadata": {
        "id": "KZjvZ2KJmNT2"
      },
      "id": "KZjvZ2KJmNT2",
      "execution_count": 46,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "bert_for_sequence_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}