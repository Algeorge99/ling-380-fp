{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a9a6afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "import json\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a435352a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999999it [00:17, 56740.74it/s] \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "filename = 'yelp_academic_dataset_review.json'\n",
    "target_user_ids = dict()\n",
    "target_item_ids = dict()\n",
    "reviews = []\n",
    "i = 0\n",
    "with open(filename, 'rt') as f:\n",
    "        for line in tqdm(f):\n",
    "            data = json.loads(line)\n",
    "\n",
    "            reviews.append({\n",
    "                key: data[key]\n",
    "                for key in ['review_id', 'user_id', 'business_id', 'stars', \"text\"]\n",
    "            })\n",
    "            target_user_ids[data['user_id']] = True\n",
    "            target_item_ids[data['business_id']] = True\n",
    "            i+=1\n",
    "            if i == 1000000:\n",
    "                break\n",
    "                \n",
    "review_df = pd.DataFrame(reviews)\n",
    "review_df[\"stars\"] = review_df[\"stars\"].astype(np.int32)\n",
    "del reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d32cbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv (r'yelp_balanced.csv', index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30aa711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpusSentsTagged:\n",
    "    def __init__(self, data, test_data = False, include_stars = False):\n",
    "        self.test_data = test_data\n",
    "        self.data = data\n",
    "        self.include_stars = include_stars\n",
    "\n",
    "    def __iter__(self):\n",
    "        for ix, row in enumerate(self.data.iloc):\n",
    "            text = gensim.utils.simple_preprocess(row[\"text\"])\n",
    "            if self.test_data:\n",
    "                yield text, row[\"stars\"]\n",
    "            else:\n",
    "                if self.include_stars:\n",
    "                    yield gensim.models.doc2vec.TaggedDocument(text, [ix, row[\"stars\"]])\n",
    "                else:\n",
    "                    yield gensim.models.doc2vec.TaggedDocument(text, [ix])\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        row = self.data.iloc[key]\n",
    "        text = gensim.utils.simple_preprocess(row[\"text\"])\n",
    "        if self.test_data:\n",
    "            return text, row[\"stars\"]\n",
    "        else:\n",
    "            if self.include_stars:\n",
    "                return gensim.models.doc2vec.TaggedDocument(text, [row[\"user_id\"], row[\"stars\"]])\n",
    "            else:\n",
    "                return gensim.models.doc2vec.TaggedDocument(text, [row[\"user_id\"]])\n",
    "        \n",
    "def infer(model, text):\n",
    "    return model.infer_vector(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48d3d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = MyCorpusSentsTagged(data)\n",
    "#corpus[0]\n",
    "corpus = MyCorpusSentsTagged(review_df, include_stars = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb7820",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = MyCorpusSentsTagged(data)\n",
    "corpus = MyCorpusSentsTagged(review_df, include_stars = True)\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size = 300,\n",
    "                                      min_count = 1,\n",
    "                                      epochs = 8,\n",
    "                                      workers = -1,\n",
    "                                      min_alpha = .0025,\n",
    "                                      max_vocab_size = None)\n",
    "model.build_vocab(corpus)\n",
    "model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e07e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = [], []\n",
    "#corp = MyCorpusSentsTagged(data, True)\n",
    "corp = MyCorpusSentsTagged(review_df, True)\n",
    "for doc, stars in corp:\n",
    "    x.append(infer(model, doc))\n",
    "    y.append(stars)\n",
    "y_ = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b0061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse = False)\n",
    "enc.fit(y_.reshape(-1,1))\n",
    "y = enc.transform(y_.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbb1c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd95005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y_, test_size=.2, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(n_jobs=-1, C=1e5)\n",
    "logreg.fit(x_train, y_train)\n",
    "logreg.score(np.array(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24520525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 0.20096666666666665\n",
      "Neural Net 0.0\n",
      "OLS -0.002585289857950501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 0.00, NNZs: 300, Bias: -0.688178, T: 108000, Avg. loss: 0.538661\n",
      "Total training time: 0.18 seconds.\n",
      "Norm: 0.00, NNZs: 300, Bias: 0.146964, T: 108000, Avg. loss: 0.990244\n",
      "Total training time: 0.17 seconds.\n",
      "Norm: 0.00, NNZs: 300, Bias: -0.699612, T: 108000, Avg. loss: 0.533123\n",
      "Total training time: 0.17 seconds.\n",
      "Norm: 0.00, NNZs: 300, Bias: -0.676928, T: 108000, Avg. loss: 0.546166\n",
      "Total training time: 0.16 seconds.\n",
      "Norm: 0.00, NNZs: 300, Bias: -0.683050, T: 108000, Avg. loss: 0.541663\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "Norm: 0.00, NNZs: 300, Bias: -0.702056, T: 216000, Avg. loss: 0.523673\n",
      "Total training time: 0.51 seconds.\n",
      "Norm: 0.00, NNZs: 300, Bias: -0.696950, T: 216000, Avg. loss: 0.526613\n",
      "Total training time: 0.49 seconds.\n",
      "Norm: 0.00, NNZs: 300, Bias: 0.133110, T: 216000, Avg. loss: 0.975225\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.00, NNZs: 300, Bias: -0.713482, T: 216000, Avg. loss: 0.518176\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.00, NNZs: 300, Bias: -0.690749, T: 216000, Avg. loss: 0.531214\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 3\n",
      "-- Epoch 3\n",
      "Norm: 0.00, NNZs: 300, Bias: -0.710165, T: 324000, Avg. loss: 0.518012\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.00, NNZs: 300, Bias: -0.705061, T: 324000, Avg. loss: 0.520948\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 4\n",
      "-- Epoch 4\n",
      "Norm: 0.00, NNZs: 300, Bias: -0.721602, T: 324000, Avg. loss: 0.512523\n",
      "Total training time: 0.91 seconds.\n",
      "Norm: 0.00, NNZs: 300, Bias: 0.125006, T: 324000, Avg. loss: 0.969576\n",
      "Total training time: 0.96 seconds.\n",
      "Norm: 0.00, NNZs: 300, Bias: -0.715922, T: 432000, Avg. loss: 0.514335\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.00, NNZs: 300, Bias: -0.710820, T: 432000, Avg. loss: 0.517272\n",
      "Total training time: 1.00 seconds.\n",
      "Norm: 0.00, NNZs: 300, Bias: -0.698846, T: 324000, Avg. loss: 0.525589\n",
      "Total training time: 1.03 seconds.\n",
      "-- Epoch 4\n",
      "-- Epoch 5\n",
      "-- Epoch 5\n",
      "-- Epoch 4\n",
      "Norm: 0.00, NNZs: 300, Bias: -0.727355, T: 432000, Avg. loss: 0.508852\n",
      "Total training time: 1.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.00, NNZs: 300, Bias: -0.720387, T: 540000, Avg. loss: 0.511601\n",
      "Total training time: 1.37 seconds.\n",
      "Norm: 0.00, NNZs: 300, Bias: 0.119254, T: 432000, Avg. loss: 0.965906\n",
      "Total training time: 1.36 seconds.\n",
      "Norm: 0.00, NNZs: 300, Bias: -0.704594, T: 432000, Avg. loss: 0.521933Norm: 0.00, NNZs: 300, Bias: -0.715284, T: 540000, Avg. loss: 0.514539\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 6\n",
      "Total training time: 1.38 seconds.\n",
      "\n",
      "-- Epoch 5\n",
      "-- Epoch 6\n",
      "Norm: 0.00, NNZs: 300, Bias: -0.731817, T: 540000, Avg. loss: 0.506125\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 5\n",
      "-- Epoch 6\n",
      "Norm: 0.00, NNZs: 300, Bias: -0.724036, T: 648000, Avg. loss: 0.509422\n",
      "Total training time: 1.69 seconds.\n",
      "Convergence after 6 epochs took 1.77 seconds\n",
      "Norm: 0.00, NNZs: 300, Bias: 0.114789, T: 540000, Avg. loss: 0.963176\n",
      "Total training time: 1.80 seconds.\n",
      "Norm: 0.00, NNZs: 300, Bias: -0.718933, T: 648000, Avg. loss: 0.512361\n",
      "Total training time: 1.85 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.00, NNZs: 300, Bias: -0.735463, T: 648000, Avg. loss: 0.503951\n",
      "Total training time: 1.89 seconds.\n",
      "Norm: 0.00, NNZs: 300, Bias: -0.709051, T: 540000, Avg. loss: 0.519214\n",
      "Total training time: 1.89 seconds.\n",
      "Convergence after 6 epochs took 1.93 seconds\n",
      "-- Epoch 6\n",
      "Convergence after 6 epochs took 2.03 seconds\n",
      "Norm: 0.00, NNZs: 300, Bias: 0.111142, T: 648000, Avg. loss: 0.961000\n",
      "Total training time: 2.14 seconds.\n",
      "Norm: 0.00, NNZs: 300, Bias: -0.712695, T: 648000, Avg. loss: 0.517047\n",
      "Total training time: 2.12 seconds.\n",
      "Convergence after 6 epochs took 2.19 seconds\n",
      "Convergence after 6 epochs took 2.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM 0.2006\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "names = [\"Decision Tree\", \"Neural Net\", \"OLS\", \"Linear SVM\"]\n",
    "\n",
    "classifiers = [    \n",
    "    DecisionTreeClassifier(),                  \n",
    "    MLPClassifier(alpha=1e-5, hidden_layer_sizes=(50, 5)),\n",
    "    linear_model.LinearRegression(),\n",
    "    SGDClassifier(loss='hinge',\n",
    "                                      penalty='l2',\n",
    "                                      alpha=30,\n",
    "                                      max_iter=1000, \n",
    "                                      tol=1e-3,\n",
    "                                      shuffle=True,\n",
    "                                      verbose=1,\n",
    "                                      n_jobs=-1,\n",
    "                                      random_state=0,\n",
    "                                      learning_rate='optimal',\n",
    "                                      early_stopping=True,\n",
    "                                      class_weight='balanced')\n",
    "]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=42)\n",
    "x_train, x_test, y_train_, y_test_ = train_test_split(x, y_, test_size=.2, random_state=42)\n",
    "\n",
    "\n",
    "# iterate over classifiers\n",
    "for name, clf in zip(names, classifiers):\n",
    "    try:\n",
    "        clf.fit(np.array(x_train), y_train)\n",
    "        score = clf.score(np.array(x_test), y_test)\n",
    "    except Exception as E:\n",
    "        clf.fit(np.array(x_train), y_train_)\n",
    "        score = clf.score(np.array(x_test), y_test_)\n",
    "    print(name, score)\n",
    "\n",
    "min_score = 0\n",
    "min_val = 0\n",
    "for val in np.arange(2, 30, 15):\n",
    "    clf = KNeighborsClassifier(val)\n",
    "    clf.fit(x_train, y_train)\n",
    "    tmp = clf.score(np.array(x_test), y_test)\n",
    "    if tmp > min_score:\n",
    "        min_val = val\n",
    "        min_score = tmp\n",
    "print(\"%s Nearest Neighbors\" % min_val, min_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a5f3f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"examples_combined.csv\")\n",
    "df.dropna(inplace = True)\n",
    "df.astype({\"contradictory\" : \"int32\", \"figurative\" : \"int32\",\n",
    "           \"confusing\" : \"int32\", \"qualifier\" : \"int32\", })\n",
    "contradicting = df.loc[df[\"contradictory\"] == 1]\n",
    "confusing = df.loc[df[\"confusing\"] == 1]\n",
    "qualifier = df.loc[df[\"qualifier\"] == 1]\n",
    "figurative = df.loc[df[\"figurative\"] == 1]\n",
    "assert all(len (i) > 0 for i in [contradicting, confusing, qualifier, figurative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "163d8801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = [], []\n",
    "for doc in contradicting.iloc:\n",
    "    x.append(infer(model, [doc[\"text\"]]))\n",
    "    y.append(doc[\"stars\"])\n",
    "y = enc.transform(np.array(y).reshape(-1,1))\n",
    "classifiers[3].predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f13db6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stars(star):\n",
    "    if star > 3: return 1\n",
    "    if star == 3: return 0\n",
    "    if star < 3: return -1\n",
    "\n",
    "class MyCorpusSentsTagged:\n",
    "    def __init__(self, data, test_data = False):\n",
    "        self.test_data = test_data\n",
    "        self.data = data\n",
    "\n",
    "    def __iter__(self):\n",
    "        for row in data.iloc:\n",
    "            if self.test_data:\n",
    "                yield row[\"text\"], stars(row[\"stars\"])\n",
    "            else:\n",
    "                yield gensim.models.doc2vec.TaggedDocument(row[\"text\"], [stars(row[\"stars\"])])\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        row = self.data.iloc[key]\n",
    "        if self.test_data:\n",
    "            return row[\"text\"], stars(row[\"stars\"])\n",
    "        else:\n",
    "            return gensim.models.doc2vec.TaggedDocument(row[\"text\"], [stars(row[\"stars\"]), row[\"user_id\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6626ab28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "495b077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = MyCorpusSentsTagged(data)\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size = 400,\n",
    "                                      min_count=1,\n",
    "                                      epochs = 10,\n",
    "                                      workers = -1,\n",
    "                                      max_vocab_size = None)\n",
    "model.build_vocab(corpus)\n",
    "model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "baca6b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = [], []\n",
    "corp = MyCorpusSentsTagged(data, True)\n",
    "for doc, stars_ in corp:\n",
    "    x.append(infer(model, [doc]))\n",
    "    y.append(stars_)\n",
    "y_ = np.array(y)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse = False)\n",
    "enc.fit(y_.reshape(-1,1))\n",
    "y = enc.transform(y_.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "58b8782c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 0.36013333333333336\n",
      "Neural Net 0.0001\n",
      "OLS -0.002739648801408793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 0.00, NNZs: 400, Bias: 0.302567, T: 108000, Avg. loss: 0.955811\n",
      "Total training time: 0.11 seconds.\n",
      "Norm: 0.00, NNZs: 400, Bias: -0.571030, T: 108000, Avg. loss: 0.798267\n",
      "Total training time: 0.11 seconds.\n",
      "Norm: 0.00, NNZs: 400, Bias: -0.641580, T: 108000, Avg. loss: 0.562092\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "Norm: 0.00, NNZs: 400, Bias: -0.652339, T: 216000, Avg. loss: 0.550445\n",
      "Total training time: 0.37 seconds.\n",
      "Norm: 0.00, NNZs: 400, Bias: -0.577203, T: 216000, Avg. loss: 0.796011\n",
      "Total training time: 0.38 seconds.\n",
      "Norm: 0.00, NNZs: 400, Bias: 0.296429, T: 216000, Avg. loss: 0.953601\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 3\n",
      "-- Epoch 3\n",
      "-- Epoch 3\n",
      "Norm: 0.00, NNZs: 400, Bias: -0.580813, T: 324000, Avg. loss: 0.795168\n",
      "Total training time: 0.65 seconds.\n",
      "Norm: 0.00, NNZs: 400, Bias: -0.658638, T: 324000, Avg. loss: 0.546056\n",
      "Total training time: 0.65 seconds.\n",
      "Norm: 0.00, NNZs: 400, Bias: 0.292837, T: 324000, Avg. loss: 0.952770\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 4\n",
      "-- Epoch 4\n",
      "-- Epoch 4\n",
      "Norm: 0.00, NNZs: 400, Bias: -0.583376, T: 432000, Avg. loss: 0.794620\n",
      "Total training time: 0.94 seconds.\n",
      "Norm: 0.00, NNZs: 400, Bias: 0.290281, T: 432000, Avg. loss: 0.952232\n",
      "Total training time: 0.94 seconds.\n",
      "Norm: 0.00, NNZs: 400, Bias: -0.663109, T: 432000, Avg. loss: 0.543205\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 5\n",
      "-- Epoch 5\n",
      "-- Epoch 5\n",
      "Norm: 0.00, NNZs: 400, Bias: -0.585364, T: 540000, Avg. loss: 0.794213\n",
      "Total training time: 1.21 seconds.\n",
      "Norm: 0.00, NNZs: 400, Bias: 0.288300, T: 540000, Avg. loss: 0.951830\n",
      "Total training time: 1.22 seconds.\n",
      "Norm: 0.00, NNZs: 400, Bias: -0.666578, T: 540000, Avg. loss: 0.541083\n",
      "Total training time: 1.23 seconds.\n",
      "-- Epoch 6\n",
      "-- Epoch 6\n",
      "-- Epoch 6\n",
      "Norm: 0.00, NNZs: 400, Bias: -0.586988, T: 648000, Avg. loss: 0.793888\n",
      "Total training time: 1.50 seconds.\n",
      "Norm: 0.00, NNZs: 400, Bias: 0.286682, T: 648000, Avg. loss: 0.951509\n",
      "Total training time: 1.52 seconds.\n",
      "Norm: 0.00, NNZs: 400, Bias: -0.669415, T: 648000, Avg. loss: 0.539391\n",
      "Total training time: 1.55 seconds.\n",
      "Convergence after 6 epochs took 1.57 seconds\n",
      "Convergence after 6 epochs took 1.63 seconds\n",
      "Convergence after 6 epochs took 1.66 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    1.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM 0.3977\n",
      "6 Nearest Neighbors 0.14186666666666667\n"
     ]
    }
   ],
   "source": [
    "names = [\"Decision Tree\", \"Neural Net\", \"OLS\", \"Linear SVM\"]\n",
    "\n",
    "classifiers = [    \n",
    "    DecisionTreeClassifier(),                  \n",
    "    MLPClassifier(alpha=1e-5, hidden_layer_sizes=(50, 5)),\n",
    "    linear_model.LinearRegression(),\n",
    "    SGDClassifier(loss='hinge',\n",
    "                                      penalty='l2',\n",
    "                                      alpha=30,\n",
    "                                      max_iter=1000, \n",
    "                                      tol=1e-3,\n",
    "                                      shuffle=True,\n",
    "                                      verbose=1,\n",
    "                                      n_jobs=-1,\n",
    "                                      random_state=0,\n",
    "                                      learning_rate='optimal',\n",
    "                                      early_stopping=True,\n",
    "                                      class_weight='balanced')\n",
    "]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=42)\n",
    "x_train, x_test, y_train_, y_test_ = train_test_split(x, y_, test_size=.2, random_state=42)\n",
    "\n",
    "\n",
    "# iterate over classifiers\n",
    "for name, clf in zip(names, classifiers):\n",
    "    try:\n",
    "        clf.fit(np.array(x_train), y_train)\n",
    "        score = clf.score(np.array(x_test), y_test)\n",
    "    except Exception as E:\n",
    "        clf.fit(np.array(x_train), y_train_)\n",
    "        score = clf.score(np.array(x_test), y_test_)\n",
    "    print(name, score)\n",
    "\n",
    "min_score = 0\n",
    "min_val = 0\n",
    "for val in np.arange(2, 30, 4):\n",
    "    clf = KNeighborsClassifier(val)\n",
    "    clf.fit(x_train, y_train)\n",
    "    tmp = clf.score(np.array(x_test), y_test)\n",
    "    if tmp > min_score:\n",
    "        min_val = val\n",
    "        min_score = tmp\n",
    "print(\"%s Nearest Neighbors\" % min_val, min_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59ffcc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
