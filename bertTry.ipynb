{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaae6acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 00:32:14.573880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-14 00:32:14.574257: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-12-14 00:32:14.574266: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-12-14 00:32:14.574480: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Importing the relevant modules\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm, json\n",
    "#config = (vocab_size = 30522, hidden_size = 768,\n",
    "#          num_hidden_layers = 12, num_attention_heads = 12,\n",
    "#          intermediate_size = 3072, hidden_act = 'gelu',\n",
    "#          hidden_dropout_prob = 0.1. attention_probs_dropout_prob = 0.1,\n",
    "#          max_position_embeddings = 512, type_vocab_size = 2, initializer_range = 0.02,\n",
    "#          layer_norm_eps = 1e-12, pad_token_id = 0, \n",
    "#          position_embedding_type = 'absolute', use_cache = True,\n",
    "#          classifier_dropout = None**kwargs )\n",
    "\n",
    "hidden_size = 300\n",
    "#makes inference faster\n",
    "configuration = BertConfig(intermediate_size = 2048,  output_hidden_states = True,\n",
    "                           hidden_size = hidden_size, num_hidden_layers = 8)\n",
    "#delete .cuda() if you don't have a good graphics card/it isn't configured to work with training\n",
    "model = BertModel(configuration).cuda()\n",
    "\n",
    "# This is the same tokenizer that\n",
    "# was used in the model to generate\n",
    "# embeddings to ensure consistency\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2c4637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_text_preparation(text, tokenizer = tokenizer):\n",
    "    \"\"\"Preparing the input for BERT\n",
    "    \n",
    "    Takes a string argument and performs\n",
    "    pre-processing like adding special tokens,\n",
    "    tokenization, tokens to ids, and tokens to\n",
    "    segment ids. All tokens are mapped to seg-\n",
    "    ment id = 1.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to be converted\n",
    "        tokenizer (obj): Tokenizer object\n",
    "            to convert text into BERT-re-\n",
    "            adable tokens and ids\n",
    "        \n",
    "    Returns:\n",
    "        list: List of BERT-readable tokens\n",
    "        obj: Torch tensor with token ids\n",
    "        obj: Torch tensor segment ids\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    return tokenized_text, tokens_tensor, segments_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c3c5bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 17142.93it/s]\n"
     ]
    }
   ],
   "source": [
    "filename = '../yelp_academic_dataset_review.json'\n",
    "reviews = []\n",
    "with open(filename, 'rt') as f:\n",
    "        for line in tqdm.tqdm(f):\n",
    "            data = json.loads(line)\n",
    "\n",
    "            reviews.append({\n",
    "                key: data[key]\n",
    "                for key in ['review_id', 'user_id', 'business_id', 'stars', \"text\"]\n",
    "            })\n",
    "            #rapid development\n",
    "            if len(reviews) > 3: break\n",
    "review_df = pd.DataFrame(reviews)\n",
    "del reviews\n",
    "\n",
    "for cat in review_df.columns:\n",
    "    if cat != \"stars\":\n",
    "        review_df[cat] = review_df[cat].astype(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a69ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
    "    \"\"\"Get embeddings from an embedding model\n",
    "    \n",
    "    Args:\n",
    "        tokens_tensor (obj): Torch tensor size [n_tokens]\n",
    "            with token ids for each token in text\n",
    "        segments_tensors (obj): Torch tensor size [n_tokens]\n",
    "            with segment ids for each token in text\n",
    "        model (obj): Embedding model to generate embeddings\n",
    "            from token and segment ids\n",
    "    \n",
    "    Returns:\n",
    "        list: List of list of floats of size\n",
    "            [n_tokens, n_embedding_dimensions]\n",
    "            containing embeddings for each token\n",
    "    \n",
    "    \"\"\"\n",
    "    # Gradient calculation id disabled\n",
    "    # Model is in inference mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        # Removing the first hidden state\n",
    "        # The first state is the input state\n",
    "        hidden_states = outputs[2][1:]\n",
    "\n",
    "    # Getting embeddings from the final BERT layer\n",
    "    token_embeddings = hidden_states[-1]\n",
    "    # Collapsing the tensor into 1-dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
    "    # Converting torchtensors to lists\n",
    "    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
    "\n",
    "    return list_token_embeddings\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "#needed to pad sequences :P\n",
    "text = \"[SEP]\"\n",
    "tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)\n",
    "SEP_EMBEDDING = get_bert_embeddings(tokens_tensor.cuda(), segments_tensors.cuda(), model)[0]\n",
    "\n",
    "def convert_sentence_to_list_embeddings(text, pad_to, tokenizer = tokenizer, seq_length = 512):\n",
    "    tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)\n",
    "    ret = []\n",
    "    tokens = chunks(tokens_tensor[0].numpy(), seq_length)\n",
    "    segments = chunks(segments_tensors[0].numpy(), seq_length)\n",
    "    for tokens_, segments_ in zip(tokens, segments):\n",
    "        ret+= get_bert_embeddings(torch.Tensor([tokens_]).int().cuda(),\n",
    "                                  torch.tensor([segments_]).int().cuda(), model)\n",
    "    ret += [SEP_EMBEDDING]*(pad_to - len(ret))\n",
    "    return np.array(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa3377ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "class MyCorpusSentsTagged(tensorflow.keras.utils.Sequence):\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(review_df) // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = review_df.iloc[index * self.batch_size:(index + 1) * self.batch_size][\"seq\"].values\n",
    "        Y = review_df[\"stars\"].values[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        X = np.array([i for i in X])\n",
    "        return X, Y/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e368c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 437.08it/s]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "#get pad size\n",
    "max_size = 0\n",
    "for text in tqdm.tqdm(review_df[\"text\"]):\n",
    "    tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)\n",
    "    max_size = max(max_size, len(tokenized_text))\n",
    "    #create new df\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "review_df[\"seq\"] = review_df[\"text\"].apply(lambda x: convert_sentence_to_list_embeddings(x, pad_to = max_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "180f52ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.67096435, -0.22226565,  0.57651962, ..., -0.08539443,\n",
       "        -0.21303857,  0.00731292],\n",
       "       [-0.11436866, -0.26132988,  0.15417067, ...,  0.26289273,\n",
       "         0.02030965, -0.26024677],\n",
       "       [-0.49506043, -0.1133729 ,  0.76526106, ...,  0.81353719,\n",
       "         0.74180205, -0.20380036],\n",
       "       ...,\n",
       "       [-0.38710659, -0.44933628,  0.55043239, ...,  0.27131868,\n",
       "        -0.42394756, -0.02236621],\n",
       "       [-0.59919821, -0.20126078,  0.28864319, ...,  0.44738151,\n",
       "        -0.07630822, -0.26450109],\n",
       "       [-0.62032728, -0.51285725, -0.10072138, ..., -0.20616235,\n",
       "         0.91448201,  0.24781478]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df[\"max\"] = review_df[\"seq\"].apply(lambda x: np.max(x))\n",
    "review_df[\"min\"] = review_df[\"seq\"].apply(lambda x: np.min(x))\n",
    "MIN = np.min(review_df[\"min\"].values)\n",
    "MAX = np.max(review_df[\"max\"].values)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.data_min_ = MIN\n",
    "scaler.data_max_ = MAX\n",
    "scaler.min_ = .01\n",
    "scaler.scale_ = .5\n",
    "scaler.transform(review_df[\"seq\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d3c22419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.1619287 , -0.2645313 ,  1.33303924, ...,  0.00921113,\n",
       "        -0.24607714,  0.19462583],\n",
       "       [-0.04873732, -0.34265976,  0.48834134, ...,  0.70578547,\n",
       "         0.22061931, -0.34049355],\n",
       "       [-0.81012087, -0.0467458 ,  1.71052213, ...,  1.80707438,\n",
       "         1.66360409, -0.22760071],\n",
       "       ...,\n",
       "       [-0.59421318, -0.71867256,  1.28086479, ...,  0.72263737,\n",
       "        -0.66789513,  0.13526758],\n",
       "       [-1.01839643, -0.22252156,  0.75728638, ...,  1.07476301,\n",
       "         0.02738356, -0.34900217],\n",
       "       [-1.06065457, -0.8457145 , -0.02144276, ..., -0.23232471,\n",
       "         2.00896401,  0.67562957]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6aeb36ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.371129035949707"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14194af5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 - 135s - loss: 31.1483 - 135s/epoch - 3s/step\n",
      "Epoch 2/10\n",
      "50/50 - 180s - loss: 7.8346 - 180s/epoch - 4s/step\n",
      "Epoch 3/10\n",
      "50/50 - 227s - loss: 1.5103 - 227s/epoch - 5s/step\n",
      "Epoch 4/10\n",
      "50/50 - 248s - loss: 0.6775 - 248s/epoch - 5s/step\n",
      "Epoch 5/10\n",
      "50/50 - 282s - loss: 0.4291 - 282s/epoch - 6s/step\n",
      "Epoch 6/10\n",
      "50/50 - 310s - loss: 0.3199 - 310s/epoch - 6s/step\n",
      "Epoch 7/10\n",
      "50/50 - 354s - loss: 0.2688 - 354s/epoch - 7s/step\n",
      "Epoch 8/10\n",
      "50/50 - 355s - loss: 0.2401 - 355s/epoch - 7s/step\n",
      "Epoch 9/10\n",
      "50/50 - 327s - loss: 0.2172 - 327s/epoch - 7s/step\n",
      "Epoch 10/10\n",
      "50/50 - 296s - loss: 0.2055 - 296s/epoch - 6s/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 1158, 75)          112800    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 75)                45300     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 76        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 158,176\n",
      "Trainable params: 158,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import metrics\n",
    "import keras\n",
    "from keras.regularizers import L1L2\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=1e-2, patience=5, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=True)\n",
    "\n",
    "batch_size = 128\n",
    "data_generator = MyCorpusSentsTagged(batch_size)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.LSTM(75, input_shape = (max_size, hidden_size), return_sequences=True, kernel_regularizer=L1L2(l1=0.01, l2=0.01)))\n",
    "model.add(keras.layers.LSTM(75, kernel_initializer='he_uniform'))\n",
    "model.add(keras.layers.Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(data_generator, epochs=epochs, verbose=2)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fdf6e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7858824 ]\n",
      " [0.7858822 ]\n",
      " [0.78588223]\n",
      " [0.7858823 ]\n",
      " [0.78588223]\n",
      " [0.7858823 ]\n",
      " [0.7858824 ]\n",
      " [0.78588223]\n",
      " [0.7858822 ]\n",
      " [0.7858822 ]]\n",
      "[4. 4. 5. 2. 4. 1. 2. 5. 4. 5.]\n"
     ]
    }
   ],
   "source": [
    "X = review_df[\"seq\"].values[:30]\n",
    "Y = review_df[\"stars\"].values[:30]\n",
    "X = np.array([i for i in X])\n",
    "print(model.predict(X[:10]))\n",
    "print(Y[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6a18dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4b6dbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from keras import metrics\\nimport keras\\nimport tensorflow as tf\\nimport os\\n\\nX = test_df[\"seq\"].values\\nY = test_df[\"stars\"].values\\nX = np.array([i for i in X])\\nfrom sklearn.preprocessing import OneHotEncoder\\nenc = OneHotEncoder(handle_unknown=\\'ignore\\')\\nenc.fit(Y.reshape(-1,1))\\nY = enc.transform(Y.reshape(-1,1)).toarray()\\n\\nepochs = 2\\nbatch_size = 32\\nwindow_length = 20\\n\\nearly_stop = tf.keras.callbacks.EarlyStopping(\\n    monitor=\\'val_loss\\', min_delta=1e-2, patience=5, verbose=0, mode=\\'auto\\',\\n    baseline=None, restore_best_weights=True)\\n\\n\\nmodel = keras.Sequential()\\n#model.add(keras.layers.LSTM(100, dropout=0.1, recurrent_dropout=0.1, return_sequences=True))\\nmodel.add(keras.layers.Dense(50))\\nmodel.add(keras.layers.LSTM(50, kernel_initializer=\\'he_uniform\\', name=\\'encoder_2\\'))\\nmodel.add(keras.layers.Dense(5))\\nmodel.compile(loss=\\'mean_squared_error\\', optimizer=\\'adam\\')\\nmodel.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=2)\\nprint(model.summary())\\n\\n\\nprint(model.predict(X[:20]))\\nprint(Y[:20])'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from keras import metrics\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "X = test_df[\"seq\"].values\n",
    "Y = test_df[\"stars\"].values\n",
    "X = np.array([i for i in X])\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(Y.reshape(-1,1))\n",
    "Y = enc.transform(Y.reshape(-1,1)).toarray()\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 32\n",
    "window_length = 20\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=1e-2, patience=5, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=True)\n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "#model.add(keras.layers.LSTM(100, dropout=0.1, recurrent_dropout=0.1, return_sequences=True))\n",
    "model.add(keras.layers.Dense(50))\n",
    "model.add(keras.layers.LSTM(50, kernel_initializer='he_uniform', name='encoder_2'))\n",
    "model.add(keras.layers.Dense(5))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "print(model.predict(X[:20]))\n",
    "print(Y[:20])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f69cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c5ea735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff319d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831c246e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d76d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee45d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5eb8217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7919de37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
